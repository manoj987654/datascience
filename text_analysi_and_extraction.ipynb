{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQPymocRHzl0xJEfrm2QgY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoj987654/datascience/blob/main/text_analysi_and_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX62p2IMTrea",
        "outputId": "c906a85c-b12d-4052-addb-6941871917ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas requests beautifulsoup4 openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiUQlGtgTu8f",
        "outputId": "5d98dacc-56a0-47e1-b4c0-75b39139a36b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Load  input Excel file\n",
        "df_input = pd.read_excel(\"Input.xlsx\")\n",
        "\n",
        "# Create a directory to store extracted articles\n",
        "output_text_dir = \"extracted_articles\"\n",
        "os.makedirs(output_text_dir, exist_ok=True)\n",
        "\n",
        "# Function to extract article title and text\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract title (assuming it's in <title> or h1 tag)\n",
        "            title_tag = soup.find(\"title\") or soup.find(\"h1\")\n",
        "            title = title_tag.get_text().strip() if title_tag else \"No Title\"\n",
        "\n",
        "            # Extract main article text\n",
        "            article_body = soup.find(\"article\") or soup.find(\"div\", {\"class\": \"post-content\"})\n",
        "            if article_body:\n",
        "                paragraphs = article_body.find_all(\"p\")\n",
        "                article_text = \"\\n\".join(p.get_text().strip() for p in paragraphs)\n",
        "            else:\n",
        "                article_text = \"No Article Text Found\"\n",
        "\n",
        "            return title, article_text\n",
        "        else:\n",
        "            return \"Error: Unable to fetch page\", \"\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", \"\"\n",
        "\n",
        "# Loop through each URL, extract text, and save it\n",
        "for index, row in df_input.iterrows():\n",
        "    url_id = row[\"URL_ID\"]\n",
        "    url = row[\"URL\"]\n",
        "\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    # Save to text file\n",
        "    file_path = os.path.join(output_text_dir, f\"{url_id}.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"{title}\\n\\n{article_text}\")\n",
        "\n",
        "print(\"Extraction complete. Check the 'extracted_articles' folder for output files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U2xa0V3UNGe",
        "outputId": "8515adb1-f996-4200-e204-08ca447a3910"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete. Check the 'extracted_articles' folder for output files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Upload extracted text files manually in Colab\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Define stop words and positive/negative words (Upload these manually if available)\n",
        "stop_words = set()  # Load from stopwords file if available\n",
        "positive_words = set()  # Load from positive words file if available\n",
        "negative_words = set()  # Load from negative words file if available\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_and_tokenize(text):\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    return tokens\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiouy\"\n",
        "    word = word.lower()\n",
        "    syllable_count = 0\n",
        "    prev_char_was_vowel = False\n",
        "    for char in word:\n",
        "        if char in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                syllable_count += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "    if word.endswith(\"e\"):\n",
        "        syllable_count -= 1\n",
        "    return max(syllable_count, 1)\n",
        "\n",
        "# Function to compute sentiment scores\n",
        "def compute_sentiment(tokens):\n",
        "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
        "    negative_score = sum(1 for word in tokens if word in negative_words) * -1\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# Function to compute readability metrics\n",
        "def compute_readability(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = clean_and_tokenize(text)\n",
        "    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)\n",
        "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
        "    percentage_complex_words = complex_word_count / len(words) if words else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    return avg_sentence_length, percentage_complex_words, fog_index, complex_word_count\n",
        "\n",
        "# Function to compute additional linguistic metrics\n",
        "def compute_additional_metrics(text):\n",
        "    words = clean_and_tokenize(text)\n",
        "    syllable_per_word = sum(count_syllables(word) for word in words) / len(words) if words else 0\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
        "    return len(words), syllable_per_word, personal_pronouns, avg_word_length\n",
        "\n",
        "# Process uploaded files\n",
        "text_analysis_results = []\n",
        "for file_name in uploaded_files.keys():\n",
        "    try:\n",
        "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        tokens = clean_and_tokenize(text)\n",
        "        pos_score, neg_score, pol_score, subj_score = compute_sentiment(tokens)\n",
        "        avg_sent_len, perc_complex, fog_idx, comp_word_count = compute_readability(text)\n",
        "        word_count, syllables, pronouns, avg_word_len = compute_additional_metrics(text)\n",
        "\n",
        "        text_analysis_results.append([file_name, pos_score, neg_score, pol_score, subj_score,\n",
        "                                      avg_sent_len, perc_complex, fog_idx, comp_word_count,\n",
        "                                      word_count, syllables, pronouns, avg_word_len])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_name}: {str(e)}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "columns = [\"File Name\", \"Positive Score\", \"Negative Score\", \"Polarity Score\", \"Subjectivity Score\",\n",
        "           \"Avg Sentence Length\", \"Percentage of Complex Words\", \"Fog Index\", \"Complex Word Count\",\n",
        "           \"Word Count\", \"Syllable per Word\", \"Personal Pronouns\", \"Avg Word Length\"]\n",
        "df_results = pd.DataFrame(text_analysis_results, columns=columns)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"/content/Output Data Structure.csv\"\n",
        "df_results.to_csv(output_file, index=False)\n",
        "print(f\"Analysis complete. Results saved to {output_file}\")\n",
        "\n",
        "# Provide download link\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WacU0XBWUOeP",
        "outputId": "d455d72d-d427-46d1-b013-1831f8926800"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-23322488-2f0a-4b5f-80f8-1838e6d234fa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-23322488-2f0a-4b5f-80f8-1838e6d234fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Netclan20241017.txt to Netclan20241017 (1).txt\n",
            "Saving Netclan20241018.txt to Netclan20241018 (1).txt\n",
            "Saving Netclan20241019.txt to Netclan20241019 (1).txt\n",
            "Saving Netclan20241020.txt to Netclan20241020 (1).txt\n",
            "Saving Netclan20241021.txt to Netclan20241021 (1).txt\n",
            "Saving Netclan20241022.txt to Netclan20241022 (1).txt\n",
            "Saving Netclan20241023.txt to Netclan20241023 (1).txt\n",
            "Saving Netclan20241024.txt to Netclan20241024 (1).txt\n",
            "Saving Netclan20241025.txt to Netclan20241025 (1).txt\n",
            "Saving Netclan20241026.txt to Netclan20241026 (1).txt\n",
            "Saving Netclan20241027.txt to Netclan20241027 (1).txt\n",
            "Saving Netclan20241028.txt to Netclan20241028 (1).txt\n",
            "Saving Netclan20241029.txt to Netclan20241029 (1).txt\n",
            "Saving Netclan20241030.txt to Netclan20241030 (1).txt\n",
            "Saving Netclan20241031.txt to Netclan20241031 (1).txt\n",
            "Saving Netclan20241032.txt to Netclan20241032 (1).txt\n",
            "Saving Netclan20241033.txt to Netclan20241033 (1).txt\n",
            "Saving Netclan20241034.txt to Netclan20241034 (1).txt\n",
            "Saving Netclan20241035.txt to Netclan20241035 (1).txt\n",
            "Saving Netclan20241036.txt to Netclan20241036 (1).txt\n",
            "Saving Netclan20241037.txt to Netclan20241037 (1).txt\n",
            "Saving Netclan20241038.txt to Netclan20241038 (1).txt\n",
            "Saving Netclan20241039.txt to Netclan20241039 (1).txt\n",
            "Saving Netclan20241040.txt to Netclan20241040 (1).txt\n",
            "Saving Netclan20241041.txt to Netclan20241041 (1).txt\n",
            "Saving Netclan20241042.txt to Netclan20241042 (1).txt\n",
            "Saving Netclan20241043.txt to Netclan20241043 (1).txt\n",
            "Saving Netclan20241044.txt to Netclan20241044 (1).txt\n",
            "Saving Netclan20241045.txt to Netclan20241045 (1).txt\n",
            "Saving Netclan20241046.txt to Netclan20241046 (1).txt\n",
            "Saving Netclan20241047.txt to Netclan20241047 (1).txt\n",
            "Saving Netclan20241048.txt to Netclan20241048 (1).txt\n",
            "Saving Netclan20241049.txt to Netclan20241049 (1).txt\n",
            "Saving Netclan20241050.txt to Netclan20241050 (1).txt\n",
            "Saving Netclan20241051.txt to Netclan20241051 (1).txt\n",
            "Saving Netclan20241052.txt to Netclan20241052 (1).txt\n",
            "Saving Netclan20241053.txt to Netclan20241053 (1).txt\n",
            "Saving Netclan20241054.txt to Netclan20241054 (1).txt\n",
            "Saving Netclan20241055.txt to Netclan20241055 (1).txt\n",
            "Saving Netclan20241056.txt to Netclan20241056 (1).txt\n",
            "Saving Netclan20241057.txt to Netclan20241057 (1).txt\n",
            "Saving Netclan20241058.txt to Netclan20241058 (1).txt\n",
            "Saving Netclan20241059.txt to Netclan20241059 (1).txt\n",
            "Saving Netclan20241060.txt to Netclan20241060 (1).txt\n",
            "Saving Netclan20241061.txt to Netclan20241061 (1).txt\n",
            "Saving Netclan20241062.txt to Netclan20241062 (1).txt\n",
            "Saving Netclan20241063.txt to Netclan20241063 (1).txt\n",
            "Saving Netclan20241064.txt to Netclan20241064 (1).txt\n",
            "Saving Netclan20241065.txt to Netclan20241065 (1).txt\n",
            "Saving Netclan20241066.txt to Netclan20241066 (1).txt\n",
            "Saving Netclan20241067.txt to Netclan20241067 (1).txt\n",
            "Saving Netclan20241068.txt to Netclan20241068 (1).txt\n",
            "Saving Netclan20241069.txt to Netclan20241069 (1).txt\n",
            "Saving Netclan20241070.txt to Netclan20241070 (1).txt\n",
            "Saving Netclan20241071.txt to Netclan20241071 (1).txt\n",
            "Saving Netclan20241072.txt to Netclan20241072 (1).txt\n",
            "Saving Netclan20241073.txt to Netclan20241073 (1).txt\n",
            "Saving Netclan20241074.txt to Netclan20241074 (1).txt\n",
            "Saving Netclan20241075.txt to Netclan20241075 (1).txt\n",
            "Saving Netclan20241076.txt to Netclan20241076 (1).txt\n",
            "Saving Netclan20241077.txt to Netclan20241077 (1).txt\n",
            "Saving Netclan20241078.txt to Netclan20241078 (1).txt\n",
            "Saving Netclan20241079.txt to Netclan20241079 (1).txt\n",
            "Saving Netclan20241080.txt to Netclan20241080 (1).txt\n",
            "Saving Netclan20241081.txt to Netclan20241081 (1).txt\n",
            "Saving Netclan20241082.txt to Netclan20241082 (1).txt\n",
            "Saving Netclan20241083.txt to Netclan20241083 (1).txt\n",
            "Saving Netclan20241084.txt to Netclan20241084 (1).txt\n",
            "Saving Netclan20241085.txt to Netclan20241085 (1).txt\n",
            "Saving Netclan20241086.txt to Netclan20241086 (1).txt\n",
            "Saving Netclan20241087.txt to Netclan20241087 (1).txt\n",
            "Saving Netclan20241088.txt to Netclan20241088 (1).txt\n",
            "Saving Netclan20241089.txt to Netclan20241089 (1).txt\n",
            "Saving Netclan20241090.txt to Netclan20241090 (1).txt\n",
            "Saving Netclan20241091.txt to Netclan20241091 (1).txt\n",
            "Saving Netclan20241092.txt to Netclan20241092 (1).txt\n",
            "Saving Netclan20241093.txt to Netclan20241093 (1).txt\n",
            "Saving Netclan20241094.txt to Netclan20241094 (1).txt\n",
            "Saving Netclan20241095.txt to Netclan20241095 (1).txt\n",
            "Saving Netclan20241096.txt to Netclan20241096 (1).txt\n",
            "Saving Netclan20241097.txt to Netclan20241097 (1).txt\n",
            "Saving Netclan20241098.txt to Netclan20241098 (1).txt\n",
            "Saving Netclan20241099.txt to Netclan20241099 (1).txt\n",
            "Saving Netclan20241100.txt to Netclan20241100 (1).txt\n",
            "Saving Netclan20241101.txt to Netclan20241101 (1).txt\n",
            "Saving Netclan20241102.txt to Netclan20241102 (1).txt\n",
            "Saving Netclan20241103.txt to Netclan20241103 (1).txt\n",
            "Saving Netclan20241104.txt to Netclan20241104 (1).txt\n",
            "Saving Netclan20241105.txt to Netclan20241105 (1).txt\n",
            "Saving Netclan20241106.txt to Netclan20241106 (1).txt\n",
            "Saving Netclan20241107.txt to Netclan20241107 (1).txt\n",
            "Saving Netclan20241108.txt to Netclan20241108 (1).txt\n",
            "Saving Netclan20241109.txt to Netclan20241109 (1).txt\n",
            "Saving Netclan20241110.txt to Netclan20241110 (1).txt\n",
            "Saving Netclan20241111.txt to Netclan20241111 (1).txt\n",
            "Saving Netclan20241112.txt to Netclan20241112 (1).txt\n",
            "Saving Netclan20241113.txt to Netclan20241113 (1).txt\n",
            "Saving Netclan20241114.txt to Netclan20241114 (1).txt\n",
            "Saving Netclan20241115.txt to Netclan20241115 (1).txt\n",
            "Saving Netclan20241116.txt to Netclan20241116 (1).txt\n",
            "Saving Netclan20241117.txt to Netclan20241117 (1).txt\n",
            "Saving Netclan20241118.txt to Netclan20241118 (1).txt\n",
            "Saving Netclan20241119.txt to Netclan20241119 (1).txt\n",
            "Saving Netclan20241120.txt to Netclan20241120 (1).txt\n",
            "Saving Netclan20241121.txt to Netclan20241121 (1).txt\n",
            "Saving Netclan20241122.txt to Netclan20241122 (1).txt\n",
            "Saving Netclan20241123.txt to Netclan20241123 (1).txt\n",
            "Saving Netclan20241124.txt to Netclan20241124 (1).txt\n",
            "Saving Netclan20241125.txt to Netclan20241125 (1).txt\n",
            "Saving Netclan20241126.txt to Netclan20241126 (1).txt\n",
            "Saving Netclan20241127.txt to Netclan20241127 (1).txt\n",
            "Saving Netclan20241128.txt to Netclan20241128 (1).txt\n",
            "Saving Netclan20241129.txt to Netclan20241129 (1).txt\n",
            "Saving Netclan20241130.txt to Netclan20241130 (1).txt\n",
            "Saving Netclan20241131.txt to Netclan20241131 (1).txt\n",
            "Saving Netclan20241132.txt to Netclan20241132 (1).txt\n",
            "Saving Netclan20241133.txt to Netclan20241133 (1).txt\n",
            "Saving Netclan20241134.txt to Netclan20241134 (1).txt\n",
            "Saving Netclan20241135.txt to Netclan20241135 (1).txt\n",
            "Saving Netclan20241136.txt to Netclan20241136 (1).txt\n",
            "Saving Netclan20241137.txt to Netclan20241137 (1).txt\n",
            "Saving Netclan20241138.txt to Netclan20241138 (1).txt\n",
            "Saving Netclan20241139.txt to Netclan20241139 (1).txt\n",
            "Saving Netclan20241140.txt to Netclan20241140 (1).txt\n",
            "Saving Netclan20241141.txt to Netclan20241141 (1).txt\n",
            "Saving Netclan20241142.txt to Netclan20241142 (1).txt\n",
            "Saving Netclan20241143.txt to Netclan20241143 (1).txt\n",
            "Saving Netclan20241144.txt to Netclan20241144 (1).txt\n",
            "Saving Netclan20241145.txt to Netclan20241145 (1).txt\n",
            "Saving Netclan20241146.txt to Netclan20241146 (1).txt\n",
            "Saving Netclan20241147.txt to Netclan20241147 (1).txt\n",
            "Saving Netclan20241148.txt to Netclan20241148 (1).txt\n",
            "Saving Netclan20241149.txt to Netclan20241149 (1).txt\n",
            "Saving Netclan20241150.txt to Netclan20241150 (1).txt\n",
            "Saving Netclan20241151.txt to Netclan20241151 (1).txt\n",
            "Saving Netclan20241152.txt to Netclan20241152 (1).txt\n",
            "Saving Netclan20241153.txt to Netclan20241153 (1).txt\n",
            "Saving Netclan20241154.txt to Netclan20241154 (1).txt\n",
            "Saving Netclan20241155.txt to Netclan20241155 (1).txt\n",
            "Saving Netclan20241156.txt to Netclan20241156 (1).txt\n",
            "Saving Netclan20241157.txt to Netclan20241157 (1).txt\n",
            "Saving Netclan20241158.txt to Netclan20241158 (1).txt\n",
            "Saving Netclan20241159.txt to Netclan20241159 (1).txt\n",
            "Saving Netclan20241160.txt to Netclan20241160 (1).txt\n",
            "Saving Netclan20241161.txt to Netclan20241161 (1).txt\n",
            "Saving Netclan20241162.txt to Netclan20241162 (1).txt\n",
            "Saving Netclan20241163.txt to Netclan20241163 (1).txt\n",
            "Analysis complete. Results saved to /content/Output Data Structure.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b4e78ff2-e078-45ef-8ea4-9b8c977154fc\", \"Output Data Structure.csv\", 19293)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}